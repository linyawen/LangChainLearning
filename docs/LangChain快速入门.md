[TOC]

# 一、诞生背景

随着OpenAI的GPT, Anthropic  Claude等 大模型兴起，众多开发者在快速构建基于大模型应用时遇到了不少挑战：

> 1. **LLMs 的局限性**：LLMs 本质上是通用文本生成模型：
>    - 缺乏特定领域的知识和私有数据。
>    - 难以处理复杂的逻辑和多步骤任务。
>    - 输出具有不确定性和幻觉风险。
>    - 上下文窗口有限，无法处理大量信息。
> 2. **“搭积木”的需求**：开发者意识到，要构建真正有用、强大且可靠的应用，需要将 LLM 与其他组件结合起来：
>    - **外部数据源**：搜索（Google）、数据库（SQL、NoSQL）、文档（PDFs, Word, Notion）、API（维基百科、公司系统）。
>    - **逻辑与工具**：需要执行计算、运行代码、调用外部 API、利用传统算法。
>    - **状态管理**：支持多轮对话、记住上下文。
>    - **抽象层**：避免对特定 LLM 的 API 细节过度耦合，便于切换模型或组合多个模型。

**LangChain 的核心愿景就是：提供一个统一的、抽象化的框架，让开发者能够轻松地将 LLM 与外部知识源、外部工具和外部环境（记忆、状态）连接起来，构建出端到端的复杂应用。**

> 说人话： LangChain是最世界主流的大模型应用开发框架，
> 就像 springboot,springcloud是世界主流的后端开发框架 。

# 二、关键特性

学习编写demo时，覆盖如下这些关键特性，就基本掌握了这个框架：

- **链(Chain)**：定义顺序步骤（LLM 调用 -> 数据处理 -> 下一个 LLM 调用）。
- **表达式语法**（LCEL）：大大简化了链和代理的构建，并内置了并发、流式传输、重试等实用功能。
- **代理**(Agent)：让 LLM 能够自主思考（Reasoning）并调用外部工具执行任务。
- **记忆(Memory)**：支持对话历史、摘要、实体记忆等多种形式。
- **文档加载器与检索器**：无缝接入各种格式的文档并构建检索增强生成（**RAG**）流程。
- **回调/追踪**：方便调试和监控应用执行流程

# 三、生态

**集成爆发**：几乎覆盖了所有主流 LLM、云数据库、嵌入式模型框架（如 Ollama）、工具服务。

**LangSmith (监控与调试平台)**：推出了官方商业平台 LangSmith，提供强大的跟踪、监控、评估、部署等功能，极大地解决了实际部署中的调试和优化难题。

**LangGraph (状态机编排)**：引入更复杂的循环、分支控制能力，用于构建更高级的自主代理或工作流。

> **总结发展特点：快速迭代响应社区需求，概念引领（代理、RAG普及化），功能不断丰富强大（LCEL、LangGraph），走向生产化和商业化（v0.1/v1.0, LangSmith）。**

# 四、国内环境

**LangChain 是国内众多大模型的统一关键接入层**。开发者可以通过 LangChain 轻松切换不同国产模型进行开发、测试和部署。这是其在国内的核心价值之一。

> （如 DeepSeek Chat/Coder, 通义千问, 文心一言, 月之暗面的 Kimi/Glimmer, 智谱 AI ChatGLM, 百川, 讯飞星火等）

# 五、Ollama + LangChain 

## (一)大模型私有化部署

Ollama： 个人学习/小公司最佳的部署工具。

**Ollama 的关键价值**：

1. **简化本地部署**：
   - 自动下载模型（`ollama pull llama3`）
   - 内置 GPU 加速（CUDA/Metal 支持）
   - 开箱即用的 API 服务（端口 11434）
2. **统一运行环境**：
   - 封装模型推理库（llama.cpp, text-generation-webui 等）
   - 屏蔽 CUDA 版本、驱动兼容等底层问题
3. **生产级特性**：
   - 模型热加载
   - 多模型并行
   - 长上下文优化（支持 128K）

## (二)Ollama 运行 LLM 的核心四步流程

```shell
# 1. 下载模型（自动识别平台+硬件）
ollama pull llama3:8b  # 下载8B参数版本

# 2. 启动服务（后台运行）
ollama serve &  # 启动本地API服务（默认端口11434）

# 3. 命令行交互（验证模型）
ollama run llama3:8b
>>> 为什么天空是蓝色的？  # 输入问题
>>> 天空呈现蓝色是因为... # 模型回复
```

## (三)生产高级部署架构

对于生产环境，Ollama 支持分布式部署。